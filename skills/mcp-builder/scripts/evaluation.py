"""MCP æœåŠ¡å™¨è¯„ä¼°å·¥å…·

æ­¤è„šæœ¬é€šè¿‡ä½¿ç”¨ Claude è¿è¡Œæµ‹è¯•é—®é¢˜æ¥è¯„ä¼° MCP æœåŠ¡å™¨ã€‚
"""

import argparse
import asyncio
import json
import re
import sys
import time
import traceback
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Any

from anthropic import Anthropic

from connections import create_connection

EVALUATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è®¿é—®å·¥å…·çš„ AI åŠ©æ‰‹ã€‚

å½“ç»™å®šä»»åŠ¡æ—¶ï¼Œä½ å¿…é¡»ï¼š
1. ä½¿ç”¨å¯ç”¨å·¥å…·å®Œæˆä»»åŠ¡
2. æä¾›ä½ æ–¹æ³•ä¸­æ¯ä¸ªæ­¥éª¤çš„æ‘˜è¦ï¼ŒåŒ…è£…åœ¨ <summary> æ ‡ç­¾ä¸­
3. æä¾›å¯¹æ‰€æä¾›å·¥å…·çš„åé¦ˆï¼ŒåŒ…è£…åœ¨ <feedback> æ ‡ç­¾ä¸­
4. æä¾›ä½ çš„æœ€ç»ˆå“åº”ï¼ŒåŒ…è£…åœ¨ <response> æ ‡ç­¾ä¸­

æ‘˜è¦è¦æ±‚ï¼š
- åœ¨ <summary> æ ‡ç­¾ä¸­ï¼Œä½ å¿…é¡»è§£é‡Šï¼š
  - ä½ å®Œæˆä»»åŠ¡æ‰€é‡‡å–çš„æ­¥éª¤
  - ä½ ä½¿ç”¨äº†å“ªäº›å·¥å…·ï¼ŒæŒ‰ä»€ä¹ˆé¡ºåºï¼Œä¸ºä»€ä¹ˆ
  - ä½ æä¾›ç»™æ¯ä¸ªå·¥å…·çš„è¾“å…¥
  - ä½ ä»æ¯ä¸ªå·¥å…·æ”¶åˆ°çš„è¾“å‡º
  - ä½ å¦‚ä½•å¾—å‡ºå“åº”çš„æ‘˜è¦

åé¦ˆè¦æ±‚ï¼š
- åœ¨ <feedback> æ ‡ç­¾ä¸­ï¼Œæä¾›å¯¹å·¥å…·çš„å»ºè®¾æ€§åé¦ˆï¼š
  - è¯„è®ºå·¥å…·åç§°ï¼šå®ƒä»¬æ˜¯å¦æ¸…æ™°å’Œæè¿°æ€§ï¼Ÿ
  - è¯„è®ºè¾“å…¥å‚æ•°ï¼šå®ƒä»¬æ˜¯å¦æœ‰è‰¯å¥½çš„æ–‡æ¡£ï¼Ÿå¿…éœ€å’Œå¯é€‰å‚æ•°æ˜¯å¦æ¸…æ™°ï¼Ÿ
  - è¯„è®ºæè¿°ï¼šå®ƒä»¬æ˜¯å¦å‡†ç¡®æè¿°äº†å·¥å…·çš„åŠŸèƒ½ï¼Ÿ
  - è¯„è®ºå·¥å…·ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°çš„ä»»ä½•é”™è¯¯
  - è¯†åˆ«å…·ä½“çš„æ”¹è¿›é¢†åŸŸå¹¶è§£é‡Šä¸ºä»€ä¹ˆå®ƒä»¬ä¼šæœ‰å¸®åŠ©
  - åœ¨å»ºè®®ä¸­è¦å…·ä½“å’Œå¯æ“ä½œ

å“åº”è¦æ±‚ï¼š
- ä½ çš„å“åº”åº”è¯¥ç®€æ´å¹¶ç›´æ¥å›ç­”æ‰€é—®çš„é—®é¢˜
- å§‹ç»ˆå°†æœ€ç»ˆå“åº”åŒ…è£…åœ¨ <response> æ ‡ç­¾ä¸­
- å¦‚æœæ— æ³•è§£å†³ä»»åŠ¡è¿”å› <response>NOT_FOUND</response>
- å¯¹äºæ•°å­—å“åº”ï¼Œåªæä¾›æ•°å­—
- å¯¹äº IDï¼Œåªæä¾› ID
- å¯¹äºåç§°æˆ–æ–‡æœ¬ï¼Œæä¾›è¯·æ±‚çš„ç¡®åˆ‡æ–‡æœ¬
- ä½ çš„å“åº”åº”è¯¥æ”¾åœ¨æœ€å"""


def parse_evaluation_file(file_path: Path) -> list[dict[str, Any]]:
    """è§£æå¸¦æœ‰ qa_pair å…ƒç´ çš„ XML è¯„ä¼°æ–‡ä»¶ã€‚"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        evaluations = []

        for qa_pair in root.findall(".//qa_pair"):
            question_elem = qa_pair.find("question")
            answer_elem = qa_pair.find("answer")

            if question_elem is not None and answer_elem is not None:
                evaluations.append({
                    "question": (question_elem.text or "").strip(),
                    "answer": (answer_elem.text or "").strip(),
                })

        return evaluations
    except Exception as e:
        print(f"è§£æè¯„ä¼°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™ï¼š{e}")
        return []


def extract_xml_content(text: str, tag: str) -> str | None:
    """ä» XML æ ‡ç­¾ä¸­æå–å†…å®¹ã€‚"""
    pattern = rf"<{tag}>(.*?)</{tag}>"
    matches = re.findall(pattern, text, re.DOTALL)
    return matches[-1].strip() if matches else None


async def agent_loop(
    client: Anthropic,
    model: str,
    question: str,
    tools: list[dict[str, Any]],
    connection: Any,
) -> tuple[str, dict[str, Any]]:
    """ä½¿ç”¨ MCP å·¥å…·è¿è¡Œä»£ç†å¾ªç¯ã€‚"""
    messages = [{"role": "user", "content": question}]

    response = await asyncio.to_thread(
        client.messages.create,
        model=model,
        max_tokens=4096,
        system=EVALUATION_PROMPT,
        messages=messages,
        tools=tools,
    )

    messages.append({"role": "assistant", "content": response.content})
    tool_metrics = {}

    while response.stop_reason == "tool_use":
        tool_use = next(block for block in response.content if block.type == "tool_use")
        tool_name = tool_use.name
        tool_input = tool_use.input

        tool_start_ts = time.time()
        try:
            tool_result = await connection.call_tool(tool_name, tool_input)
            tool_response = json.dumps(tool_result) if isinstance(tool_result, (dict, list)) else str(tool_result)
        except Exception as e:
            tool_response = f"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™ï¼š{str(e)}\n"
            tool_response += traceback.format_exc()
        tool_duration = time.time() - tool_start_ts

        if tool_name not in tool_metrics:
            tool_metrics[tool_name] = {"count": 0, "durations": []}
        tool_metrics[tool_name]["count"] += 1
        tool_metrics[tool_name]["durations"].append(tool_duration)

        messages.append({
            "role": "user",
            "content": [{
                "type": "tool_result",
                "tool_use_id": tool_use.id,
                "content": tool_response,
            }]
        })

        response = await asyncio.to_thread(
            client.messages.create,
            model=model,
            max_tokens=4096,
            system=EVALUATION_PROMPT,
            messages=messages,
            tools=tools,
        )
        messages.append({"role": "assistant", "content": response.content})

    response_text = next(
        (block.text for block in response.content if hasattr(block, "text")),
        None,
    )
    return response_text, tool_metrics


async def run_evaluation(eval_path: Path, connection: Any, model: str = "claude-3-7-sonnet-20250219") -> str:
    """ä½¿ç”¨ MCP æœåŠ¡å™¨å·¥å…·è¿è¡Œè¯„ä¼°ã€‚"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°")

    client = Anthropic()
    tools = await connection.list_tools()
    print(f"ğŸ“‹ ä» MCP æœåŠ¡å™¨åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")

    qa_pairs = parse_evaluation_file(eval_path)
    print(f"ğŸ“‹ åŠ è½½äº† {len(qa_pairs)} ä¸ªè¯„ä¼°ä»»åŠ¡")

    results = []
    for i, qa_pair in enumerate(qa_pairs):
        print(f"å¤„ç†ä»»åŠ¡ {i + 1}/{len(qa_pairs)}")
        start_time = time.time()
        response, tool_metrics = await agent_loop(client, model, qa_pair["question"], tools, connection)
        
        response_value = extract_xml_content(response, "response")
        summary = extract_xml_content(response, "summary")
        feedback = extract_xml_content(response, "feedback")
        
        results.append({
            "question": qa_pair["question"],
            "expected": qa_pair["answer"],
            "actual": response_value,
            "score": int(response_value == qa_pair["answer"]) if response_value else 0,
            "total_duration": time.time() - start_time,
            "tool_calls": tool_metrics,
            "summary": summary,
            "feedback": feedback,
        })

    correct = sum(r["score"] for r in results)
    accuracy = (correct / len(results)) * 100 if results else 0

    report = f"""
# è¯„ä¼°æŠ¥å‘Š

## æ‘˜è¦

- **å‡†ç¡®ç‡**: {correct}/{len(results)} ({accuracy:.1f}%)
- **æ€»å·¥å…·è°ƒç”¨æ¬¡æ•°**: {sum(sum(len(m["durations"]) for m in r["tool_calls"].values()) for r in results)}

---
"""

    for i, result in enumerate(results):
        report += f"""
### ä»»åŠ¡ {i + 1}

**é—®é¢˜**: {result["question"]}
**é¢„æœŸç­”æ¡ˆ**: `{result["expected"]}`
**å®é™…ç­”æ¡ˆ**: `{result["actual"] or "N/A"}`
**æ­£ç¡®**: {"âœ…" if result["score"] else "âŒ"}

---
"""

    return report


async def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨æµ‹è¯•é—®é¢˜è¯„ä¼° MCP æœåŠ¡å™¨")
    parser.add_argument("eval_file", type=Path, help="è¯„ä¼° XML æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-t", "--transport", choices=["stdio", "sse", "http"], default="stdio")
    parser.add_argument("-m", "--model", default="claude-3-7-sonnet-20250219")
    parser.add_argument("-c", "--command", help="è¿è¡Œ MCP æœåŠ¡å™¨çš„å‘½ä»¤")
    parser.add_argument("-a", "--args", nargs="+", help="å‘½ä»¤å‚æ•°")
    parser.add_argument("-e", "--env", nargs="+", help="KEY=VALUE æ ¼å¼çš„ç¯å¢ƒå˜é‡")
    parser.add_argument("-u", "--url", help="MCP æœåŠ¡å™¨ URL")
    parser.add_argument("-H", "--header", nargs="+", dest="headers")
    parser.add_argument("-o", "--output", type=Path)

    args = parser.parse_args()

    headers = {}
    if args.headers:
        for h in args.headers:
            if ":" in h:
                k, v = h.split(":", 1)
                headers[k.strip()] = v.strip()

    env_vars = {}
    if args.env:
        for e in args.env:
            if "=" in e:
                k, v = e.split("=", 1)
                env_vars[k.strip()] = v.strip()

    connection = create_connection(
        transport=args.transport,
        command=args.command,
        args=args.args,
        env=env_vars or None,
        url=args.url,
        headers=headers or None,
    )

    async with connection:
        report = await run_evaluation(args.eval_file, connection, args.model)
        if args.output:
            args.output.write_text(report, encoding='utf-8')
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {args.output}")
        else:
            print(report)


if __name__ == "__main__":
    asyncio.run(main())
